{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Support** : \n",
    "* Sutton & Barto *Chapter 3 : Finite Markov decision process (MDP)*\n",
    "* Sutton & Barto *Chapter 4 : Dynamic programming* \n",
    "\n",
    "# Discrete environnement and MDP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import trange\n",
    "import gym\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Introduction\n",
    "\n",
    "Some definitions :\n",
    "* **policy ($\\pi$)** : defines the learning agent's way of behaving at a given time and state\n",
    "* **state ($s_t \\in \\mathcal{S}$)** : numeric representation of what a agent is observing at a particular point of time in a given environnement\n",
    "* **action ($a_t \\in \\mathcal{A}(s)$)** : the input the agent provides to the environnement, it is choosen by applying a policy given the current state\n",
    "* **reward ($r_t \\in \\mathcal{R(s,a)} \\subset \\mathbb{R}$)** : signal returned by the environnement reflecting how well the agent is performing\n",
    "\n",
    "MDPs are a formalization of sequential decision making where actions influence both immediate rewards and future rewards through subsequent states.\n",
    "\n",
    "![title](img/schema.jpg)\n",
    "\n",
    "In a finite MDP, $\\mathcal{S}$, $\\mathcal{A}$ and $\\mathcal{R}$ all have a finite number of elements, random variables $S_t$ and $R_t$ define discrete probability distributions depending on the previous state and action. The probability of ending in state $s'$ and getting a reward $r$ at step $t$ is then given as :\n",
    "\n",
    "\\begin{cases} \n",
    "p(s',r|s,a) = p(S_t=s', R_t=r|S_{t-1}=s, A_{t-1}=a) \\\\  \n",
    "\\sum_{s' \\in \\mathcal{S}} \\sum_{r \\in \\mathcal{R}} p(s',r|s,a) = 1 \\\\\n",
    "p: \\mathcal{S} \\times \\mathcal{R} \\times \\mathcal{S} \\times \\mathcal{A} \\to [0,1] \n",
    "\\end{cases}\n",
    "\n",
    "The transition probability of ending in state $s'$ from state $s$ is given by : \n",
    "\n",
    "$$p(s'|s,a) = \\sum_{r \\in \\mathcal{R}} p(s',r|s,a)$$\n",
    "\n",
    "You can also compute the expected state-action reward $r(s,a) \\to \\mathbb{R}$, it is the reward you can expect after taking action $a$ in state $s$ :\n",
    "\n",
    "$$r(s,a) = \\mathbb{E}[R_t|S_{t-1}=s,A_{t-1}=a]= \\sum_{r \\in \\mathcal{R}} r \\sum_{s' \\in \\mathcal{S}} p(s',r|s,a)$$\n",
    "\n",
    "Here a representation of a simple MDP with 3 states and 2 actions :\n",
    "![title](img/mdp_1.png)\n",
    "\n",
    "### Exercice 2.1 :\n",
    "**1.a)** Using the previous schema, compute :\n",
    "\n",
    "$$p(S_0|S_1,a_0)=$$\n",
    "$$p(S_0|S_2,a_1)=$$\n",
    "$$\\mathbb{E}[R_t|S_{t-1}=S_1,A_{t-1}=a_0]=$$\n",
    "$$\\mathbb{E}[R_t|S_{t-1}=S_2,A_{t-1}=a_1]=$$\n",
    "\n",
    "**1.b)** Actions are now taken randomly : $p(a_0|S) = 1-p(a_1|S)=0.3$, compute :\n",
    "\n",
    "$$\\mathbb{E}[R_t|S_{t-1}=S_1]=$$\n",
    "$$\\mathbb{E}[R_t|S_{t-1}=S_2]=$$\n",
    "\n",
    "**2.)** You want an agent to learn to play this game :\n",
    "<img src=\"img/mario.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "* How would you define a state in this game ?\n",
    "* How many actions can you pick ?\n",
    "* How many states are possible ?\n",
    "* What kind of reward function would you set ?\n",
    "* Can we represent the game as a finite MDP ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Goals and rewards\n",
    "\n",
    "In reinforcement learning, we want to maximize the cumulative reward over the long run. To do so, we need to take into account future rewards. Lets denote $R_{t+1}, R_{t+2}, R_{t+2},\\ldots$ rewards we get after time step $t$, we want to maximize a function $G_t$ that depends on this sequence. In the simplest case :\n",
    "\n",
    "$$G_t = \\sum_{k = 0}^\\infty R_{t+k+1}$$\n",
    "\n",
    "In this form, we make no trade off between immediate and long term rewards as they are all weighted the same way. In reality this function (depending on the task) is often modified by adding a **discounting factor** denoted $\\gamma \\in [0,1]$ with its value usually close to $1$ : \n",
    "\n",
    "$$G_t = \\sum_{k = 0}^\\infty \\gamma^k R_{t+k+1}$$\n",
    "\n",
    "In a continuous task with a constant reward per time step (e.g a survival game), this sum is finite and easier to handle. We can also rewrite $G_t$ as :\n",
    "\n",
    "$$G_t = R_{t+1} + \\gamma G_{t+1}$$\n",
    "\n",
    "\n",
    "# 2.3 State and state-action value\n",
    "\n",
    "Where in the bandit problem we estimated $q_*(a)$, in MDPs we want find $q_*(s,a)$ called the **optimal state-action value** (or **q-value**)\n",
    "In MDPs we want to estimate $q_*(s,a)$ called the **optimal state-action value** (or **q-value**) or $v_*(s)$ named the **optimal state value** :\n",
    "* $v_*(s)$ : is the sum of all discounted future rewards the agent can expect on average after it reachs a state $s$ assuming he is acting optimally\n",
    "* $q_*(s,a)$ : is the sum of all discounted future rewards the agent can expect on average after it reachs a state $s$ and  takes the action $a$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 State Value and optimal State Value function\n",
    "\n",
    "Solving a reinforcement learning problem means finding a policy $\\pi$ that gives a lot of rewards in the long run. To do so, we need to estimate a **value function** (or state value function) that estimates how good it is to stay in a given state. In order to discriminate states, we compare them in term of expected future rewards. This expectation depends of course on the way the agent is acting, called the policy $\\pi$. Mathematically speaking, we can write the value function as :\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "v_\\pi (s) & = \\mathbb{E}_\\pi[R_{t+1} + \\gamma G_{t+1}|S_t=s] \\quad \\forall s \\in \\mathcal{S} \\\\\n",
    "& = \\sum_a \\pi(a|s) \\sum_{s',r}p(s'|s,a)[r + \\gamma v_\\pi(s')] \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "This fundamental equation is called the **Bellman equation** : it gives a relationship between the value of the current state and its successor states. If $V(s)$ denotes the estimated value of $v_\\pi (s)$ then it can be computed as : \n",
    "\n",
    "$$V_{t+1}(s) \\gets \\sum_a \\pi(a|s) \\sum_{s',r}p(s'|s, a)[r + \\gamma V_{t}(s')]$$\n",
    "\n",
    "In RL we want to compute the **optimal state value function** $v_* (s)$ which assume the agent is acting optimally. The value function is slightly modified to take into account this aspect :\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "v_* (s) & = \\underset{a}{\\operatorname{max}} \\mathbb{E}_\\pi[R_{t+1} + \\gamma G_{t+1}|S_t=s, A_t=a] \\quad \\forall s, a \\in \\mathcal{S}, \\mathcal{A} \\\\\n",
    "& = \\underset{a}{\\operatorname{max}} \\sum_{s',r}p(s'|s, a)[r + \\gamma v_*(s')] \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Which can be estimated using :\n",
    "\n",
    "$$V_{t+1}(s) \\gets \\underset{a}{\\operatorname{max}} \\sum_{s',r}p(s'|s, a)[r + \\gamma V_{t}(s')]$$\n",
    "\n",
    "This algorithm is called the **value iteration algorithm**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 2.2 : \n",
    "An agent moves in a grid world :\n",
    "* 4 possible actions : left, up, right, down\n",
    "* moving outside the grid gives $-1$ and cancels the move\n",
    "* if the agent is in A, he gets $+10$ and is sent to A'\n",
    "* if the agent is in B, he gets $+5$ and is sent to B'\n",
    "* else moving gives $0$\n",
    "\n",
    "<img src=\"img/gridworld.jpg\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "\n",
    "* Compute value and optimal value function\n",
    "* Can you infere a policy ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORLD_SIZE = 5\n",
    "A_POS = [0, 1]\n",
    "A_PRIME_POS = [4, 1]\n",
    "B_POS = [0, 3]\n",
    "B_PRIME_POS = [2, 3]\n",
    "DISCOUNT = 0.9\n",
    "\n",
    "# left, up, right, down\n",
    "ACTIONS = [np.array([0, -1]), #left\n",
    "           np.array([-1, 0]), #up\n",
    "           np.array([0, 1]), #right\n",
    "           np.array([1, 0])] #down\n",
    "ACTION_PROB = 0.25\n",
    "\n",
    "\n",
    "def step(state, action):\n",
    "    #If we are at point A or B, we are sent to A' or B', the step ends\n",
    "    if state == A_POS:\n",
    "        return A_PRIME_POS, 10\n",
    "    if state == B_POS:\n",
    "        return B_PRIME_POS, 5\n",
    "    \n",
    "    #Move the agent\n",
    "    next_state = (np.array(state) + action).tolist()\n",
    "    x, y = next_state\n",
    "    \n",
    "    #if we go outside the grid : -1 else 0\n",
    "    if x < 0 or x >= WORLD_SIZE or y < 0 or y >= WORLD_SIZE:\n",
    "        reward = -1.0\n",
    "        next_state = state\n",
    "    else:\n",
    "        reward = 0\n",
    "    return next_state, reward\n",
    "\n",
    "\n",
    "######################################\n",
    "###########    POLICIES    ###########\n",
    "######################################\n",
    "\n",
    "def value_function():\n",
    "    \n",
    "    \n",
    "    value = np.zeros((WORLD_SIZE, WORLD_SIZE))\n",
    "    #<Add your code here>\n",
    "    while True:\n",
    "        break\n",
    "\n",
    "def optimal_value_function():\n",
    "    \n",
    "    value = np.zeros((WORLD_SIZE, WORLD_SIZE))\n",
    "    #<Add your code here>\n",
    "    \n",
    "    while True:\n",
    "        break\n",
    "\n",
    "value_function()\n",
    "optimal_value_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2  State-action Value and optimal State-action Value function\n",
    "\n",
    "The state value function $v_\\pi$ can be seen as an expectation over possible actions for a given state $s$ and a policy $\\pi$. Formally, this can be written as :\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "v_\\pi (s) & = \\mathbb{E}_\\pi[R_{t+1} + \\gamma G_{t+1}|S_t=s] \\quad \\forall s \\in \\mathcal{S} \\\\\n",
    "& = \\sum_a \\pi(a|s) \\sum_{s',r}p(s'|s,a)[r + \\gamma v_\\pi(s')] \\\\\n",
    "& = \\sum_a \\pi(a|s) q_\\pi(s,a)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The state-action value function $q_\\pi$ is then  :\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "q_\\pi (s,a) & = \\sum_{s',r}p(s'|s,a)[r + \\gamma v_\\pi(s')] \\\\\n",
    "& = \\mathbb{E}_\\pi[R_{t+1} + \\gamma G_{t+1}|S_t=s, A_t=a] \\quad \\forall s, a \\in \\mathcal{S}, \\mathcal{A}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "When we compute the optimal state value function $v_*$, we assume the agent is acting optimally. This is linked to the **Bellman equation** which assumes that the vaue of a state under an optimal policy must equal the expected return for the best action from that state. This means that the agent will always choose the action that leads to the state with the maximum value :\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "v_* (s) & = \\underset{a}{\\operatorname{max}} \\sum_{s',r}p(s'|s, a)[r + \\gamma v_*(s')] \\\\\n",
    "& = \\underset{a}{\\operatorname{max}} q_*(s,a)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Replacing equations we can write the **optimal state-action value function** :\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "q_* (s,a) & = \\sum_{s',r}p(s'|s,a)[r + \\gamma v_*(s')] \\\\\n",
    "& = \\sum_{s',r}p(s'|s,a)[r + \\gamma \\underset{a'}{\\operatorname{max}} q_*(s',a')]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Which $Q^*_t$, its estimated value can be computed using :\n",
    "\n",
    "$$Q_{t+1}(s,a) \\gets  \\sum_{s',r}p(s'|s, a)[r + \\gamma \\underset{a'}{\\operatorname{max}}Q_t(s',a')]$$\n",
    "\n",
    "This is the **Q-value iteration** algorithm. If there is no uncertainty (deterministic transition like in grid world), the rule is reduced as :\n",
    "\n",
    "$$Q_{t+1}(s,a) \\gets  [r + \\gamma \\underset{a'}{\\operatorname{max}}Q_t(s',a')]$$\n",
    "\n",
    "This is the main component of the **Q-learning** algorithm we'll see later. \n",
    "\n",
    "Thus the greedy policy is given by :\n",
    "\n",
    "$$\\pi'(s,a) = \\underset{a}{\\operatorname{argmax}} q_* (s,a)$$\n",
    "\n",
    "### Exercice 2.3 :\n",
    "\n",
    "Winter is here. You and your friends were tossing around a frisbee at the park when you made a wild throw that left the frisbee out in the middle of the lake. The water is mostly frozen, but there are a few holes where the ice has melted. If you step into one of those holes, you'll fall into the freezing water. At this time, there's an international frisbee shortage, so it's absolutely imperative that you navigate across the lake and retrieve the disc. However, the ice is slippery, so you won't always move in the direction you intend.\n",
    "\n",
    "<img src=\"img/Frozen-Lake.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "<img src=\"img/description.png\" alt=\"Drawing\" style=\"width: 100px;\"/>\n",
    "\n",
    "Using the gym environnement :\n",
    "* Compute the optimal state value function (value_iteration function)\n",
    "* Compute the optmal state-action value function (q_value function)\n",
    "* Compute the optimal policy\n",
    "* Why the optimal action of grid[3,2] is to go down ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "env.observation_space.n \n",
    "return: \n",
    "    int: nb of possible states\n",
    "\n",
    "env.action_space.n \n",
    "return: \n",
    "    int: nb of possible actions\n",
    "\n",
    "env.P[state][action] \n",
    "return: \n",
    "    float: transition_prob, \n",
    "    int: next_state, \n",
    "    float: reward, \n",
    "    bool: done\n",
    "    \n",
    "#ACTIONS\n",
    "LEFT = 0\n",
    "DOWN = 1\n",
    "RIGHT = 2\n",
    "UP = 3\n",
    "\"\"\"\n",
    "\n",
    "def value_iteration(env, nb_of_iterations=1000, gamma = 1.0, threshold=1e-20):\n",
    "    \n",
    "    # initialize value table with zeros\n",
    "    value_table = np.zeros(env.observation_space.n)\n",
    "    \n",
    "    #<Add your code here>\n",
    "    \n",
    "    return value_table\n",
    "\n",
    "def q_value(env, nb_of_iterations=1000, gamma = 1.0, threshold=1e-20):\n",
    "     \n",
    "    # initialize q table with zeros\n",
    "    q_table = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    \n",
    "    #<Add your code here>\n",
    "    \n",
    "    return q_table\n",
    "\n",
    "\n",
    "#INITIALIZE the ENVIRONNEMENT\n",
    "env = gym.make('FrozenLake-v0')\n",
    "env.reset()\n",
    "\n",
    "#<Add your code here>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4 Dealing with partial information : solving by playing\n",
    "\n",
    "So far we assumed that the environnement was perfectly known and that could get rewards from any state at any time. In reality, we don't know transition probabilities : the agent has to explore in order to get an idea of how states are linked to each other and which states give rewards.\n",
    "\n",
    "## 2.4.1 TD learning framework\n",
    "\n",
    "**Temporal difference learning** aims to solve a MDP with only partial knowledge. We assume that the agent only knows the possibles states and actions and runs an **exploration policy** (e.g epsilon greedy) to find out transition probabilities and rewards. Recall the value **iteration algorithm** :\n",
    "\n",
    "$$V_{t+1}(s) \\gets \\underset{a}{\\operatorname{max}} \\sum_{s',r}p(s'|s, a)[r + \\gamma V_{t}(s')]$$\n",
    "\n",
    "We call the term $[r + \\gamma V(s')]$ the **TD target** as it can be shown that this term is an unbiaised estimate for $V(s)$. The **TD learning** algorithm defines an exponential smoothing between the current value and the target value given a learning rate $\\alpha$ (e.g 0.01) :\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "V_{t+1}(s) & \\gets (1-\\alpha)V_{current} + \\alpha V_{target}\\\\\n",
    "V_{t+1}(s) & \\gets (1-\\alpha)V_{t}(s) + \\alpha[r + \\gamma V_{t}(s')]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "This algorithm has some similarities with $SGD$, it is volatile and somewhat unstable as it handles one sample at a time. It is necessary to reduce the learning rate over time to reduce the bouncing effect.\n",
    "\n",
    "## 2.4.2 Using the Q-function in TD learning\n",
    "\n",
    "Using the **Q-function** gives extra informations : this tells us about how good an action is in a given state. Remplacing $V(s)$ by $Q(s,a)$ leads to a new algorithm called **State–Action–Reward–State–Action (SARSA)** :\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "Q_{t+1}(s,a) & \\gets (1-\\alpha)Q_{current} + \\alpha Q_{target}\\\\\n",
    "Q_{t+1}(s,a) & \\gets (1-\\alpha)Q_{t}(s,a) + \\alpha[r + \\gamma Q_{t}(s',a')]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "By changing the way $Q_{target}$ is calculated, we can define another variant. Setting $Q_{target} = [r + \\gamma \\underset{a'}{\\operatorname{max}}Q_t(s',a')]$, we get the famous **Q-learning** algorithm :\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "Q_{t+1}(s,a) & \\gets (1-\\alpha)Q_{current} + \\alpha Q_{target}\\\\\n",
    "Q_{t+1}(s,a) & \\gets (1-\\alpha)Q_{t}(s,a) + \\alpha[r + \\gamma \\underset{a'}{\\operatorname{max}}Q_t(s',a')]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The main difference is the use of the $\\underset{a'}{\\operatorname{max}}$ operator. In **Q-learning**, picking the action $a'$ is straightforward... but how do we pick $a'$ in the **SARSA** setup ? \n",
    "\n",
    "It's simple, we use an epsilon greedy policy to choose the action.\n",
    "\n",
    "## 2.4.3 On-policy vs off-policy learning\n",
    "\n",
    "* On-policy : SARSA\n",
    "* Off-policy : Q-learning\n",
    "\n",
    "Q-learning is off-policy because it updates its Q-values using the Q-value of the next state s′ and the greedy action $a′$. The update policy (greedy) is different than the behavior policy (epsilon greedy).\n",
    "\n",
    "SARSA is on-policy because it updates its Q-values using the Q-value of the next state s′ and the current policy's action $a′′$. The update and the behavior policy are similar (epsilon greedy).\n",
    "\n",
    "There is no distinction if we use the greedy policy.\n",
    "\n",
    "Q-learning or SARSA ?\n",
    "* Q-learning and more generally off-policy learning tend to have higher sample variance and can have troubles to converge (like SGD)\n",
    "* Q-learning tends to be greedier and takes more risks while SARSA is more conservative (can lead to a sub optimal solution). In some cases it is better to limit the risk (e.g in finance)\n",
    "* The choice can be task dependent\n",
    "\n",
    "### Exercice 2.4\n",
    "\n",
    "* Compare SARSA and Q-learning on FrozenLake-v0 and Taxi-v2 environnements.\n",
    "* Why training is easier on the taxi environnement ? \n",
    "\n",
    "For this, use the env.step(action) method :\n",
    "\n",
    "e.g : new_state, reward, done, _ = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(env, \n",
    "                num_episodes, \n",
    "                max_steps_per_episode, \n",
    "                algorithm, # {\"Q-learning\",\"SARSA\"}\n",
    "                lr = 0.1, \n",
    "                eps = 1, \n",
    "                discount_rate = 0.99, \n",
    "                max_eps = 1, \n",
    "                min_eps = 0.01, \n",
    "                eps_decay = 0.002):\n",
    "    \n",
    "    rewards_all_episodes = []\n",
    "    q_table = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "\n",
    "    for episode in trange(num_episodes):\n",
    "        \n",
    "        #Reset environnement\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        rewards_current_episode = 0\n",
    "\n",
    "        for step in range(max_steps_per_episode): \n",
    "            \n",
    "            #<Add your code here>\n",
    "            \n",
    "            if done == True: \n",
    "                break\n",
    "\n",
    "        rewards_all_episodes.append(rewards_current_episode)\n",
    "\n",
    "        # Exploration rate decay\n",
    "        eps = min_eps + (max_eps - min_eps) * np.exp(-eps_decay*episode)\n",
    "    \n",
    "    return rewards_all_episodes\n",
    "    \n",
    "\n",
    "eps = 1\n",
    "max_eps = 1\n",
    "min_eps = 0.01\n",
    "eps_decay = 0.002\n",
    "\n",
    "lr = 0.1\n",
    "discount_rate = 0.99\n",
    "\n",
    "max_steps_per_episode = 50\n",
    "num_episodes = 5000\n",
    "\n",
    "env_names = ['FrozenLake-v0', \"Taxi-v2\"]\n",
    "algorithms = [\"SARSA\", \"Q-learning\"]\n",
    "\n",
    "for env_name in env_names:\n",
    "    \n",
    "    env = gym.make(env_name)\n",
    "    all_rewards = []\n",
    "    \n",
    "    for algorithm in algorithms:\n",
    "        \n",
    "        #<Add your code here>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4.4 Extensions\n",
    "\n",
    "### 2.4.4.1 TD-lambda and eligibility trace\n",
    "\n",
    "There are two main methods for solving a MDP : \n",
    "* TD learning : is biased and sensible on the initial conditions but can be computed online (after each action)\n",
    "* Monte-Carlo methods : unbiaised but requires the episode to end before updating and usually have very high variance (sample inefficient)\n",
    "\n",
    "TD-lambda uses a trick called the **eligibility trace**, that acts like a bridge between TD and MC methods. Q-learning or SARSA, can be combined with it to obtain a more general method that may learn more efficiently. An **eligibility trace** is a temporary record of the occurrence of an event (visiting a state or taking an action), it can be seen as a memory mechanism which fades over time (like a vanilla RNN).\n",
    "\n",
    "Recall SARSA update rule (works similarly with Q-learning) :\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "Q_{t+1}(s,a) & \\gets (1-\\alpha)Q_{t}(s,a) + \\alpha[r + \\gamma Q_{t}(s',a')]\\\\\n",
    "& \\gets Q_{t}(s,a) + \\alpha[r + \\gamma Q_{t}(s',a') - Q_{t}(s,a)]\\\\\n",
    "& \\gets Q_{t}(s,a) + \\alpha \\delta_t\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The term $\\delta$ is called the **TD error**. In SARSA($\\lambda$), the eligibility trace $e(s,a)$ is added to the update rule :\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "Q_{t+1}(s,a) & \\gets Q_{t}(s,a) + \\alpha \\delta_t e_t(s,a) \\quad \\forall a,s \\in \\mathcal{S}, \\mathcal{A}\\\\\n",
    "e_t(s_t,a_t) & = \\begin{cases} \\gamma \\lambda e_{t-1}(a,s) + 1 && \\text{if } a_t = a, s_t=s\\\\ \n",
    "\\gamma \\lambda e_{t-1}(a,s) && \\text{else }\\end{cases}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "There are different ways of calculating $e(s,a)$, another common idea is to replace instead of accumulating : \n",
    "\n",
    "$$e_t(s_t,a_t) = \\begin{cases} 1 && \\text{if } a_t = a, s_t=s\\\\ \n",
    "\\gamma \\lambda e_{t-1}(a,s) && \\text{else }\\end{cases}$$\n",
    "\n",
    "In both implementations : \n",
    "* if $\\lambda = 0$ we get back to the standard SARSA algorithm. \n",
    "* If $\\lambda = 1$ we are on an online MC setup \n",
    "* If $\\lambda \\in ]0,1[$, the parameter act as a decay rate. \n",
    "\n",
    "### 2.4.4.2 Double Q-learning\n",
    "\n",
    "One of the main variant is the **Double Q-learning** algorithm which aims to reduce the bias due to the use of the ${\\operatorname{max}}$ operator. Standard **Q-learning** tends to overestimate the value of a given action, since it is used as a target, training can be volatile and so slow. To solve this, we introduce a second **Q-table** in the algorithm : at each step, we randomly decide to update either table A or table B. The target Q-value is then picked up from the other Q-table :\n",
    "* If table A is choosen :\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "a^* & = \\underset{a'}{\\operatorname{argmax}Q^A_{t}(s',a')} \\\\\n",
    "Q_{t+1}^A(s,a) & \\gets (1-\\alpha)Q^A_{t}(s,a) + \\alpha[r + \\gamma Q^B_t(s',a^*)]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "* if table B is choosen :\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "b^* & = \\underset{a'}{\\operatorname{argmax}Q^B_{t}(s',a')} \\\\\n",
    "Q_{t+1}^B(s,a) & \\gets (1-\\alpha)Q^B_{t}(s,a) + \\alpha[r + \\gamma Q^A_t(s',b^*)]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### Exercice 2.5\n",
    "\n",
    "* Implement SARSA($\\lambda$) with different values on Taxi-v2 and compare\n",
    "* Implement double Q-learning on FrozenLake-v0\n",
    "* Is double Q-learning on or off-policy ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SARSA\n",
    "\n",
    "def train_agent(env, \n",
    "                lb,\n",
    "                num_episodes, \n",
    "                max_steps_per_episode,  \n",
    "                lr = 0.1, \n",
    "                eps = 1, \n",
    "                discount_rate = 0.99, \n",
    "                max_eps = 1, \n",
    "                min_eps = 0.01, \n",
    "                eps_decay = 0.002):\n",
    "    \n",
    "    rewards_all_episodes = []\n",
    "    q_table = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    \n",
    "    for episode in trange(num_episodes):\n",
    "\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        rewards_current_episode = 0\n",
    "        e = np.zeros_like(q_table)\n",
    "        \n",
    "        for step in range(max_steps_per_episode): \n",
    "            \n",
    "            #<Add your code here>\n",
    "\n",
    "            if done == True: \n",
    "                break\n",
    "\n",
    "        rewards_all_episodes.append(rewards_current_episode)\n",
    "\n",
    "        # Exploration rate decay\n",
    "        eps = min_eps + (max_eps - min_eps) * np.exp(-eps_decay*episode)\n",
    "    \n",
    "    return rewards_all_episodes\n",
    "\n",
    "eps = 1\n",
    "max_eps = 1\n",
    "min_eps = 0.01\n",
    "eps_decay = 0.002\n",
    "\n",
    "lr = 0.1\n",
    "discount_rate = 0.99\n",
    "\n",
    "max_steps_per_episode = 50\n",
    "num_episodes = 3000\n",
    "\n",
    "lambdas = [0., 0.3, 0.7, 1.]\n",
    "env = gym.make('Taxi-v2')\n",
    "all_rewards = []\n",
    "\n",
    "for lb in lambdas:\n",
    "    #<Add your code here>\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DOUBLE Q LEARNING\n",
    "\n",
    "def train_agent(env, \n",
    "                num_episodes, \n",
    "                max_steps_per_episode,  \n",
    "                lr = 0.1, \n",
    "                eps = 1, \n",
    "                discount_rate = 0.99, \n",
    "                max_eps = 1, \n",
    "                min_eps = 0.01, \n",
    "                eps_decay = 0.002):\n",
    "    \n",
    "    rewards_all_episodes = []\n",
    "    q_table_a = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    q_table_b = np.zeros_like(q_table_a)\n",
    "    \n",
    "    for episode in trange(num_episodes):\n",
    "\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        rewards_current_episode = 0\n",
    "\n",
    "        for step in range(max_steps_per_episode): \n",
    "            \n",
    "            #<Add your code here>\n",
    "\n",
    "            if done == True: \n",
    "                break\n",
    "\n",
    "        rewards_all_episodes.append(rewards_current_episode)\n",
    "\n",
    "        # Exploration rate decay\n",
    "        eps = min_eps + (max_eps - min_eps) * np.exp(-eps_decay*episode)\n",
    "    \n",
    "    return rewards_all_episodes\n",
    "\n",
    "eps = 1\n",
    "max_eps = 1\n",
    "min_eps = 0.01\n",
    "eps_decay = 0.002\n",
    "\n",
    "lr = 0.1\n",
    "discount_rate = 0.99\n",
    "\n",
    "max_steps_per_episode = 100\n",
    "num_episodes = 10000\n",
    "\n",
    "\n",
    "env = gym.make('FrozenLake-v0')\n",
    "all_rewards = []\n",
    "\n",
    "#<Add your code here>\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
