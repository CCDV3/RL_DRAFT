{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Support** : \n",
    "* Sutton & Barto *Chapter 2 : Multi-armed bandits*\n",
    "\n",
    "\n",
    "# **K-ARMED bandit problem**\n",
    "\n",
    "Consider the following learning problem. You are faced repeatedly with a choice among k different bandits. After each choice, you receive a numerical reward choosen from a stationary probability distribution depending on your action. Your objective is to maximize the expected total rewards after $T$ actions/time steps.\n",
    "\n",
    "![title](img/bandit.png)\n",
    "\n",
    "For this task, we'll use *numpy* (numerical computation), *matplotlib* (for plots) and *tqdm* (progress bars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environnement\n",
    "\n",
    "Before starting, we need to create a custom environnement to play the bandit game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bandit(object):\n",
    "    \n",
    "    def __init__(self, k = 10, epsilon = 0.05, initial = 0, true_reward = 0):\n",
    "        \"\"\"\n",
    "        k: number of bandits\n",
    "        epsilon: exploration parameter\n",
    "        initial: initial estimation for each action\n",
    "        true_reward: base reward added to the real reward for each action\n",
    "        \"\"\"\n",
    "        self.k = k\n",
    "        self.epsilon = epsilon\n",
    "        self.initial, self.true_reward = initial, true_reward\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset the bandit game\n",
    "        \"\"\"\n",
    "        # real reward for each action : this is the average reward each bandit gives (N(0,1) distribution)\n",
    "        self.q_true = np.random.randn(self.k) + self.true_reward\n",
    "        \n",
    "        # best action overall : this is the action with the highest average reward\n",
    "        self.best_action = np.argmax(self.q_true)\n",
    "        \n",
    "        # estimation for each action : initial is 0 by default\n",
    "        # this is the initial value we give to each action (default == 0)\n",
    "        self.q_estimation = np.zeros(self.k) + self.initial\n",
    "\n",
    "        # number of chosen times for each action : usefull to compute an average\n",
    "        self.action_count = np.zeros(self.k)\n",
    "        \n",
    "        # total rewards and number of times we take the best overall action\n",
    "        self.total_rewards = 0\n",
    "        self.total_good_actions = 0\n",
    "    \n",
    "    def get_best_action(self):\n",
    "        \"\"\"\n",
    "        Takes the action with the highest q_estimation\n",
    "        If it isn't unique, pick one randomly between them\n",
    "        return: index of the best action\n",
    "        \"\"\"\n",
    "        return np.random.choice(\n",
    "            np.argwhere(self.q_estimation == self.q_estimation[np.argmax(self.q_estimation)]).flatten()\n",
    "        )\n",
    "    \n",
    "    def compute_reward(self, true_reward):\n",
    "        \"\"\"\n",
    "        This method computes the reward by adding noise to the true reward (N(0,1))\n",
    "        true_reward: true reward of the action taken\n",
    "        return: the step reward\n",
    "        \"\"\"\n",
    "        return true_reward + np.random.randn()\n",
    "    \n",
    "    def update_total_rewards(self, reward):\n",
    "        \"\"\"\n",
    "        Update the total reward\n",
    "        reward: step reward\n",
    "        \"\"\"\n",
    "        self.total_reward += reward\n",
    "    \n",
    "    def update_total_good_actions(self, action):\n",
    "        \"\"\"\n",
    "        Update the number of times the best action has been chose\n",
    "        action: index of current action\n",
    "        \"\"\"\n",
    "        if action == self.best_action:\n",
    "            self.total_good_actions += 1\n",
    "    \n",
    "    \n",
    "    #######################################\n",
    "    # We will only customize next methods #\n",
    "    #######################################\n",
    "    \n",
    "    def select_action(self):\n",
    "        \"\"\"\n",
    "        This method selects the best action given the chosen algorithm\n",
    "        return: index of action to take\n",
    "        \"\"\"\n",
    "        raise(NotImplementedError)\n",
    "            \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        This method compute the reward of this stip, updates q_estimation, update total_reward and total_good_actions\n",
    "        return: step reward, total reward, part of good actions taken\n",
    "        \"\"\"\n",
    "        raise(NotImplementedError)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Objectives in K-ARMED bandit\n",
    "\n",
    "When playing a K-armed bandit game, we want to find out **which bandit gives the best reward on average**. If we know a priori which machine is the most profitable, then the problem is trivial and taking this machine at each time step will lead to the best gain overall. We denote the action selected on time step $t \\in T$ as $A_t$, and the corresponding reward $R_t$. The value of an arbitrary action $a$ denoted $q_{*}(a)$ is the expected reward given that $a$ is selected :\n",
    "\n",
    "$$q_{*}(a) = \\mathbb{E}[R_t|A_t=a]$$\n",
    "\n",
    "In reality we do not know which bandit or action has the highest expectation, we then try to estimate/approximate $q_{*}(a)$ at each time step using informations we have up to time $t$. We denote $Q_t (a)$ the **estimated value** of action $a$ at time step $t$ and we want $Q_t (a)$ to be as close as possible from $q_{*}(a)$.\n",
    "\n",
    "## 1.2 Action-Value methods\n",
    "\n",
    "Methods for estimating the value of action are called **action-value methods**. How do we estimate the value of a given action ?\n",
    "\n",
    "We want to estimate the expectation giving the information available. The simplest way to do so is to take the average reward of action $a$ until $t-1$ (also called **sample average**) :\n",
    "\n",
    "$$Q_t (a) = \\overline{R}_{<t} = \\frac{\\text{sum of rewards when } a \\text{ is picked prior to } t}{\\text{number of times } a \\text{ is picked prior to }t} = \\frac{\\sum_{\\tau=1}^{\\tau = t-1}R_{\\tau} . \\mathbb{1}_{A_t=a}}{\\sum_{\\tau=1}^{\\tau = t-1}\\mathbb{1}_{A_t=a}}$$ \n",
    "\n",
    "$$\\mathbb{1}_{A_t=a} = 1 \\text{ if } A_t = a \\text{ else } 0$$\n",
    "\n",
    "\n",
    "### Efficient computation\n",
    "Concentrating in a single action, let $R_i$ now denote the reward received after the i*th* iteration of this action and let $Q_n(a)$ denote the estimate of its action value after it has been selected $n-1$ times :\n",
    "\n",
    "$$Q_n(a) = \\frac{R_1 + R_2 + \\cdots + R_{n-1}}{n - 1}$$\n",
    "\n",
    "This formula is easy to use but quite inefficient as you need to keep track of all previous rewards related to action $a$. We can rewrite the equation to reduce memory usage :\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "Q_{n+1}(a) & = \\frac{1}{n}\\sum^{n}_{i=1}R_i \\\\\n",
    "& = \\frac{1}{n}\\big[R_n + (n-1)\\frac{1}{n-1}\\sum^{n-1}_{i=1}R_i\\big]\\\\\n",
    "& = \\frac{1}{n}\\big[R_n + (n-1)Q_n\\big] \\\\\n",
    "& = Q_n + \\frac{1}{n}\\big[R_n - Q_n\\big]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We only need the current reward $R_n$ and $Q_{old}$ to compute $Q_{new}$ !\n",
    "\n",
    "## 1.3 Greedy and epsilon greedy policy\n",
    "\n",
    "We can now estimate $Q_t(a)$ but how do we choose an action based on this value ?\n",
    "The easiest way is to select the action which has the current highest estimated value. Mathematically, if $A_t$ denote the action taken at time $t$ then :\n",
    "\n",
    "$$A_t = \\underset{a} {\\operatorname{argmax}} Q_t(a)$$\n",
    "\n",
    "This rule is highly inefficient as there is a high probability of getting stuck in a sub optimal solution. The agent will soon start to take the same action over and over without updating the other values. This policy is called the greedy policy as we want to maximize immediate gain by taking the greedy action.\n",
    "\n",
    "In reality, we often want to experiment different machine before focusing on a single one : this is called exploration. Sacrifying short term gain for exploration can often lead in higher long term gain as there is a high probability for us to test every bandit and find out the most profitable one.\n",
    "\n",
    "The simplest method to add some kind of exploration is called a **epsilon greedy policy**. Let $\\epsilon$ denote the probability of going for exploration, at each step :\n",
    "\n",
    "$$A_t = \\begin{cases} \\underset{a} {\\operatorname{argmax}} Q_t(a), & \\text{with probability } 1-\\epsilon \n",
    "\\\\ \\text{random action}, & \\text{with probability } \\epsilon \\end{cases}$$\n",
    "\n",
    "At each step, we have a chance to randomly test a bandit and update it's value !\n",
    "If $\\epsilon$ is set to $0$, epsilon greedy becomes a greedy policy. In practice, $\\epsilon$ is an hyperparameter and is often set to a small value ($0.05$ or $0.1$).\n",
    "\n",
    "\n",
    "### Exercice 1.1 :\n",
    "Implement the greedy and the epsilon greedy policies using 4 different learning rates for a 10-Armed bandit game.\n",
    "For this you have to :\n",
    "* Complete **select_action** and **step** methods from the **BanditEpsGreedy** class below inherited from the Bandit class. Use the efficient computation formula for this.\n",
    "* Create a training loop with **500 episodes** of **250 steps** each per learning rate \n",
    "* Plot the **average reward** per time step, the **average cumulative** reward per time step and average **% of optimal actions** per time step (average over the 1000 episodes and per learning rate)\n",
    "\n",
    "**Tips** : store outputs in a tensor of shape **(n_variables_to_track, nb_learning_rate, nb_episodes, np_steps)** here **(3, 4, 1000, 500)**. Variables to track are **(reward, cumulative reward, % of optimal action)**. Means can be calculated at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BanditEpsGreedy(Bandit):\n",
    "    \n",
    "    def __init__(self, k = 10, epsilon = 5e-2, initial = 0, true_reward = 0):\n",
    "        \n",
    "        super().__init__(k, epsilon, initial, true_reward)\n",
    "        \n",
    "    def select_action(self):\n",
    "        \"\"\"\n",
    "        This method selects the best action\n",
    "        return: index of action to take\n",
    "        \"\"\"\n",
    "        #<ADD YOUR CODE HERE>\n",
    "        pass\n",
    "            \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        This method compute the reward of this stip, updates q_estimation, update total_reward and total_good_actions\n",
    "        return: step reward, cumulative reward, part of good actions taken\n",
    "        \"\"\"\n",
    "        #<ADD YOUR CODE HERE>\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training loop\n",
    "episodes = 500\n",
    "steps = 250\n",
    "epsilons = [0., 0.01, 0.05, 0.1, 0.2]\n",
    "\n",
    "outputs = np.zeros((3, len(epsilons), episodes, steps))\n",
    "\n",
    "for i,epsilon in enumerate(epsilons):\n",
    "    \n",
    "    bandit = BanditEpsGreedy(epsilon=epsilon)\n",
    "    bandit.reset()\n",
    "    \n",
    "    for episode in trange(episodes):\n",
    "        \n",
    "        for step in range(steps):\n",
    "            \n",
    "            #<ADD YOUR CODE HERE>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plots\n",
    "plt.figure(figsize=(16,12))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Dealing with a nonstationary distribution\n",
    "\n",
    "In reinforcement learning, you often face nonstationary distributions : the reward distribution can change over time. For exemple we could set a simple rule that randomly change the reward of a given bandit after 20 actions on it. To handle this kind of problem, we want to change the way rewards are weighted as a recent reward is probably more important to properly estimate the true distribution. Instead of taking the mean, we set a weighting parameter $\\alpha$ :\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "Q_{n+1} & = Q_n + \\alpha\\big[R_n - Q_n\\big] \\\\\n",
    "& = \\alpha R_n + (1-\\alpha) Q_n && \\text{exponential smoothing}\\\\\n",
    "& =  (1-\\alpha)^n Q_1 + \\sum^{n}_{i=1}\\alpha (1 - \\alpha)^{n-i}R_i\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "This is a weighted average as $(1-\\alpha)^n + \\sum^{n}_{i=1}\\alpha (1 - \\alpha)^{n-i} = 1$. The parameter $\\alpha$ is often set to a small value (e.g $0.1$) and $\\alpha \\in (0,1]$ for obvious stability reasons. If $\\alpha = 1$, we only take into account the last reward.\n",
    "\n",
    "## 1.5 Upper confidence bound (UCB algorithm)\n",
    "\n",
    "Epsilon greedy algorithm has the particularity of taking random actions without discrimination. In reality, some actions can be close to optimal and more interesting to explore rather than some others. When the set of actions is very large, it is more efficient to focus on interesting areas. UCB uses this :\n",
    "\n",
    "$$A_t = \\underset{a}{\\operatorname{argmax}} \\Big[ Q_t(a) + c \\sqrt{\\frac{\\text{ln }t}{N_t(a)}} \\Big] $$\n",
    "\n",
    "Where $N_t(a)  = {\\sum_{\\tau=1}^{\\tau = t-1}\\mathbb{1}_{A_t=a}}$ is the number of times action $a$ has been picked prior to time $t$. If $N_t(a) = 0$, action $a$ is considered a maximizing action as it has never been tested before. To be able to compute this, we usually add a small amount to the denominator (e.g $10^{-8}$). \n",
    "\n",
    "The term $c \\sqrt{\\frac{\\text{ln }t}{N_t(a)}}$ can be see as an uncertainty estimate : when action $a$ isn't picked for some steps, uncertainty increases. Logarithm is used to reduce the effect of uncertainty in the long run as its weight decreases overtime in favor of $Q_t(a)$.\n",
    "\n",
    "## 1.6 Gradient based algorithm\n",
    "\n",
    "This algorithm is the closest from modern reinforcement learning as it is based on a **softmax** function and **gradient ascent**. The objective is to learn a **relative preference** instead of purely relying on rewards. The **softmax** function (**Gibbs** or **Boltzmann** distribution) can be seen as a generalization of the **sigmoid** function for *k-classes* problems. If $H_t(a)$ is the preference for action $a$ at step $t$ and $\\pi_t(a)$ the probability of taking action $a$ at time $t$:\n",
    "\n",
    "$$Prob(A_t = a) = \\pi_t(a) = \\frac{e^{H_t(a)}}{\\sum_{b=1}^{k}e^{H_t(b)}}$$\n",
    "\n",
    "This function allow us to get for each bandit a probability of being selected at step $t$. We then sample it from a categorical distribution based on those probabilities. The learning rule is set as :\n",
    "\n",
    "$$\\begin{cases} H_{t+1}(A_t) = H_{t}(A_t) + \\alpha (R_t - \\overline{R}_{<t})(1-\\pi_t(A_t)) && \\text{for } a = A_t\\\\ \n",
    "H_{t+1}(a) = H_{t}(a) - \\alpha (R_t - \\overline{R}_{<t})\\pi_t(a) && \\text{for } a \\ne A_t \\end{cases}$$\n",
    "\n",
    "The term $\\overline{R}_{<t}$ is called a baseline and helps reducing variance, forcing $H_{new}$ to not be too far from $H_{old}$. If $R_t > \\overline{R}_{<t}$, the probability of taking this action in the future is increased and also decreased for all other actions.\n",
    "\n",
    "### Exercice 1.2 :\n",
    "Implement the greedy epsilon ($\\epsilon = 0.1$), the UCB and the gradient algorithm with and without baseline, then compare them :\n",
    "* Complete **select_action** and **step** methods from the **BanditComparison** class below inherited from the Bandit class. \n",
    "* Use the method for nonstationary ($\\alpha = 0.1$)\n",
    "* method = 'UCB' is for UCB, method = 'epsilon' is for epsilon greedy algorithm, method = 'gradient' for the gradient one\n",
    "* You can reuse pieces of code from before\n",
    "* We want to plot **average rewards** per time step but you can plot the other variables also"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BanditComparison(Bandit):\n",
    "    \n",
    "    def __init__(self, k=10, epsilon=5e-2, initial=0, true_reward=0, alpha=0.1, method='UCB', c=1, baseline=False):\n",
    "        \"\"\"\n",
    "        alpha: step size parameter\n",
    "        method: UCB or epsilon or gradient -> perform UCB alogrithm or epsilon greedy algorithm\n",
    "        c: c parameter of UCB\n",
    "        baseline: using a baseline if method = \"gradient\"\n",
    "        \"\"\"\n",
    "        super().__init__(k, epsilon, initial, true_reward)\n",
    "        \n",
    "        self.alpha = alpha\n",
    "        self.method = method\n",
    "        self.c = c\n",
    "        self.baseline = baseline\n",
    "        \n",
    "    def select_action(self):\n",
    "        \"\"\"\n",
    "        This method selects the best action\n",
    "        return: index of action to take\n",
    "        \"\"\"\n",
    "        if self.method == 'UCB':\n",
    "            #<ADD YOUR CODE HERE>\n",
    "            pass\n",
    "        \n",
    "        elif self.method == 'epsilon':\n",
    "            #<ADD YOUR CODE HERE>\n",
    "            pass\n",
    "        \n",
    "        elif self.method == 'gradient':\n",
    "            #<ADD YOUR CODE HERE>\n",
    "            pass\n",
    "            \n",
    "            \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        This method compute the reward of this stip, updates q_estimation, update total_reward and total_good_actions\n",
    "        return: step reward, cumulative reward, part of good actions taken\n",
    "        \"\"\"\n",
    "        if self.method == 'UCB':\n",
    "            #<ADD YOUR CODE HERE>\n",
    "            pass\n",
    "        \n",
    "        elif self.method == 'epsilon':\n",
    "            #<ADD YOUR CODE HERE>\n",
    "            pass\n",
    "        \n",
    "        elif self.method == 'gradient':\n",
    "            if self.baseline:\n",
    "                #<ADD YOUR CODE HERE>\n",
    "                pass\n",
    "            \n",
    "            else:\n",
    "                #<ADD YOUR CODE HERE>\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon, c = 0.1, 2\n",
    "episodes = 1000\n",
    "steps = 500\n",
    "methods = [\"UCB\", \"epsilon\", \"gradient\", \"gradient\"]\n",
    "\n",
    "outputs = np.zeros((3, len(methods), episodes, steps))\n",
    "\n",
    "for i,method in enumerate(methods):\n",
    "    \n",
    "    bandit = BanditComparison(#<ADD YOUR CODE HERE>)\n",
    "        \n",
    "    bandit.reset()\n",
    "    \n",
    "    for episode in trange(episodes):\n",
    "        \n",
    "        for step in range(steps):\n",
    "            \n",
    "            #<ADD YOUR CODE HERE>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plots\n",
    "plt.figure(figsize=(16,12))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
